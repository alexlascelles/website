<html>

    <head>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,700,800">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
        
        <title>Alex Lascelles - Research</title>
        <link rel="icon" href="logo_al_white_noborder.png">
        <link rel="stylesheet" href="styles.css">
    </head>

    <body>
        <div class="container-fluid">
            <div class="row">
                <div class="col-xs-12">
                
                    <br/>
                    <ul>
                        <li class="home-icon"><a href="../index.html"><span class="home-color fa fa-home"></span></a></li>
                        <li><a href="about.html">About</a></li>
                        <li><a class="blue-title" href="research.html">Research</a></li>
                        <li><a href="cv.html">CV</a>
                        <li><a href="music_and_art.html">Music and Art</a></li>
                        <li><a href="contact.html">Contact</a></li>
                    </ul>

                    <br/>
                    <hr class="dividers" />

                    <!--
                    <div class="row">
                        <div class="col-md-12 col-xs-12">
                            <p class="blue-title-bigger">Research Interests</p>
                    
                            <p>I'm interested in a number of different topics within computational and cognitive neuroscience. Firstly, I'm intrigued by human memory, 
                                especially in the visual and auditory domains. Once we understand exactly how memories are encoded, stored, and recalled, I'm interested 
                                to know if we could then augment these processes. You could imagine a world where we are able to "back up" our memories to the cloud, and 
                                even share our memories with others.
                            </p>
                            <p>
                                It also fascinates me how we process and perceive visual and auditory information concurrently. Often, we use information received in the 
                                auditory domain to help us process a visual cue, and vice versa. Although this sometimes leads to negative biases, I wonder if it can be 
                                utilized in a way that enhances information uptake &#8212; perhaps for use in a learning environment. Additionally, I'm interested in using 
                                imaging techniques such as fMRI and MEG to combine my experience with crossmodal correspondences with memory research. The advent of machine 
                                learning and computer vision also presents many interesting opportunities to combine with neuroscientific data which I'm eager to explore.
                            </p>
                            <p>
                                Although I've switched fields, I've kept my love for Astrophysics! I enjoy reading the latest news, and hope someday I'll have the opportunity 
                                to research supernovae once more.
                            </p>
                        </div>
                    </div> -->

                    <br>
                    <center><hr class="dividers-small" /></center>
                    <p class="blue-title-bigger">Past Projects</p>
                    <center><hr class="dividers-small" /></center>
                    <br>

                    <div class="row">
                        <div class="col-12">

                            <div class="row">
                                <div class="col-md-6 col-xs-12">
                                    <p class="textwhite-large">The Algonauts Project 2023</p> 
                                    <p class="textwhite">How the Human Brain Makes Sense of Natural Scenes</p>
                                    <p>
                                        The 2023 edition of <a class="textlink" href="#Algonauts2019">The Algonauts Challenge</a> focused on explaining responses in the human brain 
                                        as participants perceive complex natural visual scenes. Through collaboration with the 
                                        <a class="textlink" href="https://naturalscenesdataset.org/">Natural Scenes Dataset (NSD)</a> team, this Challenge ran on the largest suitable 
                                        brain dataset available (brain responses from 8 human participants to in total 73,000 different visual scenes) opening new venues for data-hungry 
                                        modeling. The challenge was organized in partnership with the 
                                        <a class="textlink" href="https://ccneuro.org/" target="_blank">Conference on Cognitive Computational Neuroscience (CCN)</a>.
                                        <br><br>
                                        At every blink our eyes are flooded by a massive array of photons &#8212; and yet, we perceive the visual world as ordered and meaningful. 
                                        The primary target of the 2023 Challenge was predicting human brain responses to complex natural visual scenes. We posed the question: Given a set 
                                        of images, how well does your computational model account for the brain activations when a human viewed those images?
                                        <br><br>
                                        <a class="textlink" href="http://algonauts.csail.mit.edu/index.html" target="_blank">Learn more about Algonauts 2023 here</a>.
                                    </p>
                                </div>

                                <div class="col-md-6 col-xs-12">
                                    <iframe src="https://www.youtube.com/embed/KlwSDpxUX6k?vq=hd1080&amp;rel=0" width="560" height="315" title="The Algonauts Project 2023: Challenge &amp; development kit tutorial walk-through" frameborder="0" allowfullscreen="" data-dashlane-frameid="350"></iframe>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-md-6 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="GoalGrounding.gif" alt="goalgrounding" height="400">
                                        <div class="middle">
                                            <div class="thetext">
                                                Example of the online tool we built.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                
                                <div class="col-md-6 col-xs-12">
                                    <p class="textwhite-large">Goal Grounding Project</p>
                                    <p class="textwhite">Group collaboration between teams at MIT and Facebook Reality Labs</p>
                                    <p>
                                        The aim of this project was to collect annotations on egocentric (first-person) videos that have a specific goal which is being carried out. 
                                        This results in visual ground truth data for when a certain goal is taking place within a video. This would be useful for a number of different 
                                        applications, for example, an AI assistant predicting and helping with the next action/goal a human will encounter.
                                        <br><br>
                                        To do this, our team at MIT collaborated with a team at Meta to build an interface to collect data from participants online, selected suitable data sets 
                                        to sample from, collected the data using the Prolific platform, and analyzed the data. The interface was a useful online tool that allowed a user to quickly 
                                        select portions of video while watching, and was flexible enough to be applied to other research aims.
                                        <br><br>
                                        <a class="textlink" href="contact.html" target="_blank">If you'd like to know more about this project please get in touch</a>.
                                    </p>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-xl-7 col-xs-12">
                                    <p class="textwhite-large">The Algonauts Project 2021</p> 
                                    <p class="textwhite">How the Human Brain Makes Sense of a World in Motion</p>
                                    <p>
                                        The 2nd installment of <a class="textlink" href="#Algonauts2019">The Algonauts Challenge</a> was centered around video perception and event understanding, 
                                        and was organized in partnership with the <a class="textlink" href="https://ccneuro.org/" target="_blank">Conference on Cognitive Computational Neuroscience (CCN)</a>. 
                                        The challenge focused on explaining responses in the human brain as subjects watched short video clips of everyday events.
                                        <br><br>
                                        For this release, in addition to building the website, I was also involved in the collection of the data set &#8212; creating the video data set (1102 3-second videos), 
                                        learning fMRI techniques, scanning 10+ participants, and making all the data available to the public.
                                        <br><br>
                                        <a class="textlink" href="http://algonauts.csail.mit.edu/2021/index.html" target="_blank">Learn more about Algonauts 2021 here</a>.
                                    </p>
                                </div>

                                <div class="col-xl-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="algonauts2021.png" alt="algonauts2021" height="250">
                                    </div>
                                    <div class="picframe">
                                        <img class="pic" src="ap2021_data_figure_training_v2_compressed.gif" alt="algonauts2021_gif" height="200">
                                        <div class="middle">
                                            <div class="thetext">
                                                The data set consisted of a set of 1000 3-second videos of everyday events, for which we also provided the fMRI brain 
                                                data associated with humans viewing these videos.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-md-6 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="PaperSummary.png" alt="paper_summary" height="400">
                                        <div class="middle">
                                            <div class="thetext">
                                                Example of one of the academic paper summaries.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                
                                <div class="col-md-6 col-xs-12">
                                    <p class="textwhite-large">Computational Neuroscience Paper Summaries</p> 
                                    <p>
                                        I created one-slide summaries of 100+ computational visual neuroscience academic papers, and other resources. It was made so that you could quickly understand the 
                                        gist of the paper/resources, and easily come back to it later to remember what it was about. They included a short section on the background, info 
                                        on what the researchers did, what they found, and any other important information.
                                    </p>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row" id="Algonauts2019">
                                <div class="col-md-5 col-xs-12">
                                    <p class="textwhite-large">The Algonauts Project 2019</p> 
                                    <p class="textwhite">Explaining the Human Visual Brain: Workshop and Challenge</p>
                                    <p>
                                        While at MIT, I helped to build The Algonauts Project &#8212; 
                                        an initiative bringing biological and artificial intelligence researchers together on a 
                                        common platform to exchange ideas and advance both fields. Inspired by the astronauts' exploration of space, "algonauts" explore human and artificial intelligence 
                                        with state-of-the-art algorithmic tools. <a class="textlink" href="http://algonauts.csail.mit.edu/2019/index.html" target="_blank">Our first challenge and workshop, 
                                        "Explaining the Human Visual Brain"</a>, focused on building computer vision models that simulate how the brain sees and recognizes <i>objects</i>, a topic that 
                                        has long fascinated neuroscientists and computer scientists.
                                        <br><br>
                                        Across two competition tracks, fMRI and MEG, we gave participants a set of images consisting of everyday objects and the corresponding brain activity recorded 
                                        while human subjects viewed those images. Participants competed to devise computational models that best predicted the brain activity of a brand new set of images. 
                                        The winners were announced at a workshop that we organized at MIT, featuring guest speakers who are some of the leading experts in the field of computational 
                                        neuroscience of vision, from institutions such as Harvard, UC Berkeley, Columbia, FU Berlin, and DeepMind.
                                        <br><br>
                                        <a class="textlink" href="http://algonauts.csail.mit.edu/archive.html" target="_blank">Learn more about The Algonauts Project and the 2019 edition here, 
                                        on the webpages that I built to explain and distribute the challenges and workshops</a>.
                                    </p>
                                </div>
                                
                                <div class="col-md-7 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="algonauts2019.png" alt="algonauts2019" height="250">
                                    </div>
                                    <div class="picframe">
                                        <img class="pic" src="challenge_summary_v3.png" alt="algonauts2019_summary" height="400">
                                        <div class="middle">
                                            <div class="thetext">
                                                Summary figure explaining the 2019 challenge. 
                                                <br><br>
                                                The data set consisted of a set of 92 and 118 images, for which we also provided the fMRI and MEG brain 
                                                data associated with humans viewing these images.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>


                            <div class="row">
                                <div class="col-xl-6 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="msc.png" alt="msc" height="280"/>
                                        <div class="middle">
                                            <div class="thetext">
                                                Topoplots showing brain activity during trials where the spoken English words "up" and "down" did bias participants' perception of 
                                                motion (congruent) and where they did not (incongruent).
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                
                                <div class="col-xl-6 col-xs-12">
                                    <p class="textwhite-large">MSc Thesis (2018):</p>
                                    <p class="textwhite">Neural Correlates of Crossmodal Correspondence Between Pitch and Visual Motion</p>
                                    <p>Supervisor: Prof Joydeep Bhattacharya FRSA</p>
                                    <p>
                                        Our senses are not independent entities &#8212; instead, they work together to build our realities. When one sense influences another, we call this 
                                        phenomenon <i>crossmodal correspondence</i>. I studied a particular crossmodal correspondence in which ascending/descending pitches as well as the 
                                        spoken words "up"/"down" (auditory domain) were able to bias our perception of visual motion (visual domain).
                                        <br><br>    
                                        I learned MATLAB so I could design and conduct an EEG experiment in which the participant judged the direction of motion of an ambiguous (same 
                                        upwards and downwards components) moving grating whilst listening to the auditory stimuli. Using behavioural and ERP analysis, I showed that 
                                        auditory stimuli were enough to bias peoples' perception of visual motion &#8212; an ascending tone caused more people to perceive upwards motion, 
                                        when in fact there was no overall movement in either direction. This was the first time that this effect was studied using brain imaging techniques.
                                        <br><br>
                                        <a class="textlink" href="msc_thesis_alex_lascelles.pdf" target="_blank">Read my MSc thesis here</a> or 
                                        <a class="textlink" href="msc_poster_aal.pdf" target="_blank">view my poster for the behavioural data here</a>.
                                    </p>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-md-7 col-xs-12">
                                    <p class="textwhite-large">Speed Reader</p> 
                                    <p class="textwhite">Anvil Hack 2018</p>
                                    <p>
                                        While at Goldsmiths, I participated in a number of different hackathons including Anvil Hack. During this event, I worked in a team of 3 to make a platform 
                                        that allows you to convert text into a form that was much more quickly readable.  
                                        <a class="textlink" href="https://youtu.be/QlxADlERFkA" target="_blank">We were inspired by this advert by Honda</a>. Perhaps in the future, devices that 
                                        use this technology might be commonplace.
                                    <!-- You can try it out by visiting our <a class="textlink" href="https://github.com/lnfiniteMonkeys/Anvil_IV" target="_blank">GitHub repository</a>. -->
                                    </p>
                                </div>
                                
                                <div class="col-md-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="speedreader.gif" alt="speed_reader" height="400">
                                        <div class="middle">
                                            <div class="thetext">
                                                An example of this technology in practice. Your reading speed is greatly increased when your eyes don't have to make multiple saccades from word to word, 
                                                but can instead remain focused in one place.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-xl-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="3d_cinematic.png" alt="MHD" height="400">
                                        <div class="middle">
                                            <div class="thetext">
                                                A close up of one of my simulations. <br/><br/>The magnetic field lines are represented in grey and wind speed by the 
                                                red/black color map. The blue color map shows mass density (effectively tracing accretion).
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <br/>
                                <div class="col-xl-7 col-xs-12">
                                    <p class="textwhite-large">MPhys Thesis (2017):</p>
                                    <p class="textwhite">Realistic MHD Modeling of Wind-Driven Processes in Cataclysmic Variable-Like Binaries</p>
                                    <p>Supervisors: Dr Cecilia Garraffo and Dr Jeremy Drake</p>
                                    <p>
                                        When main sequence stars such as our Sun orbit a star known as a white dwarf, they can form something called a cataclysmic variable-like binary. The 
                                        charged ions contained within the magnetic fields of these stars creates a complex magnetic <i>wind</i> structure as the two bodies orbit each other. 
                                        When this wind structure decouples from the system it carries away mass and angular momentum, which impacts how the stars spin and how their orbits evolve. 
                                        <br><br>
                                        Using 3-D magnetohydrodynamic simulations, I explored the effects of orbital separation and magnetic field configuration on these mass and angular momentum 
                                        loss rates for binary systems. The current models didn't include the magnetic fields of the stars. It was hypothesized that including the magnetic field of 
                                        the white dwarf in the binary system could alter these rates. I found that when including these magnetic fields, the mass and angular momentum loss rates 
                                        dropped by factors of 4 and 6 respectively, suggesting that the current models for these systems should be amended to include the magnetic field.
                                        <br><br>
                                        <a class="textlink" href="mphys_thesis_alex_lascelles.pdf" target="_blank">Read my MPhys thesis here</a> 
                                        or <a class="textlink" href="https://youtu.be/G3KCS8HvReI?t=2110">watch a 20-min video of my thesis defense here</a>, given at the Harvard-Smithsonian 
                                        Center for Astrophysics High Energy Seminar.
                                    </p>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-md-7 col-xs-12">
                                    <p class="textwhite-large">Hack @ Brown 2017:</p> 
                                    <p class="textwhite">Saguaro: An open source, extensible home automation platform</p>
                                    <p>
                                        I attended a hackathon at Brown University where I worked within a team of 5 to design and build an IOT platform called Saguaro. Saguaro allows 
                                        hardware devices to be controlled by a simple web interface. <br><br>
                                        Activating hardware in your home &#8212; lights, windows, anything you can run a wire to &#8212; is as simple as pushing a button. Additionally, 
                                        Saguaro learns your schedule, and over time will be able to anticipate your actions and automate your home for you.
                                    </p>
                                    <p>
                                        <a class="textlink" href="https://devpost.com/software/saguaro" target="_blank">Learn more about this project here</a>.</p>
                                </div>

                                <div class="col-md-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="saguaro.jpg" alt="saguaro" height="300">
                                        <div class="middle">
                                            <div class="thetext">
                                                A prototype of our IoT home! Built using an Arduino.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>
                        
                            <div class="row">
                                <div class="col-md-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="orbits.png" alt="orbits"  height="250">
                                        <div class="middle">
                                            <div class="thetext">
                                                Some of the different orbits that I discovered, shown in stationary (left) and rotating (right) frames of reference.
                                            </div>
                                        </div>
                                    </div>
                                    <br/>
                                    <div class="picframe">
                                        <img class="pic" src="binary.jpg" alt="binary"  height="300">
                                        <div class="middle">
                                            <div class="thetext">
                                                Artist's rendition of an Earth-like planet orbiting a binary star system.
                                            </div>
                                        </div>
                                    </div>
                                    <br/>
                                    <p class="text-small">Bottom image credit: Lynette Cook / <a class="textlink" href="https://extrasolar.spaceart.org/" target="_blank">extrasolar.spaceart.org</a></p>
                                </div>

                                <div class="col-md-7 col-xs-12">
                                    <p class="textwhite-large">Planetary Orbits Around Binary Star Systems</p> 
                                    <p class="textwhite">Computing Project #1 (2016)</p>
                                    <p>
                                        Incredibly, almost half of the star systems that we see in the sky contain multiple stars! This project aimed to simulate planetary orbits around 
                                        a binary (two) star system. This is important to study because such simulations could be used to detect habitable planets outside our solar system (exoplanets). 
                                        For life to exist, the planet on which it occurs must keep a stable orbit over long time-scales. This is similar to the novel "The Three-Body Problem" 
                                        by Liu Cixen, now made into a <a class="textlink" href="https://www.youtube.com/watch?v=mogSbMD6EcY&ab_channel=Netflix" target="_blank">Netflix series</a>. <br><br>
                                        To find stable orbits, I used a Runge-Kutta approach to solve the differential equations involved in such a 3-body system and tested various initial 
                                        velocities and separations. I found many different p- and s-type orbits (the two species of stable planetary orbit), as well as some chaotic orbits, 
                                        and discovered whether they possessed a habitable zone where the planet could sustain life.
                                    </p>
                                    <p>
                                        <a class="textlink" href="binary_project.pdf" target="_blank">Read my report on this project here</a>.</p>
                                    <br/>
                                    <br/>                        
                                </div>
                            </div>
                            
                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-md-7 col-xs-12">
                                    <p class="textwhite-large">The Structure of White Dwarf Stars</p> 
                                    <p class="textwhite">Computing Project #2 (2016)</p>
                                    <p>
                                        White dwarf (WD) stars are extremely dense objects &#8212; the only thing preventing them from collapsing into a black hole is a force called 
                                        <a class="textlink" href="https://en.wikipedia.org/wiki/Electron_degeneracy_pressure" target="_blank">electron degeneracy pressure</a>, which 
                                        has to do with the fact that electrons cannot be pushed close enough to occupy the same energy state. WDs are very important within astronomy 
                                        &#8212; they appear in many areas of study, including galaxy formation, stellar evolution, and supernovae. Since over 95% of the stars we 
                                        observe will end their lives as WDs, knowing how they function and how their parameters behave is crucial. <br><br>
                                        In this project, I created a model for determining the mass-radius variation in WDs, and ultimately found the critical mass of a WD 
                                        (<a class="textlink" href="https://en.wikipedia.org/wiki/Chandrasekhar_limit" target="_blank">Chandrasekhar limit</a>), beyond which the 
                                        electron degeneracy pressure can no longer support the star and it must collapse into a neutron star or a black hole.
                                    </p>
                                    <p ><a class="textlink" href="wd_project.pdf" target="_blank">Read more about this project here</a>.</p>                      
                                </div>

                                <div class="col-md-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="white_dwarf2.jpg" alt="wd"  height="200">
                                        <div class="middle">
                                            <div class="thetext">
                                                Artist's rendition of a white dwarf star.
                                            </div>
                                        </div>
                                    </div>
                                    <br/>
                                    <div class="picframe">
                                        <img class="pic" src="wd.png" alt="myWD"  height="200">
                                        <div class="middle">
                                            <div class="thetext">
                                                Radius vs Mass: Plot shows how parameters of real WDs (black and red) compare to my model of Carbon and Iron WDs.
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <br/>
                            <center><hr class="dividers-smallish" /></center>
                            <br/>

                            <div class="row">
                                <div class="col-lg-5 col-xs-12">
                                    <div class="picframe">
                                        <img class="pic" src="submarine2.png" alt="submarine2" height="250"> 
                                        <div class="middle">
                                            <div class="thetext">
                                                An exmaple of a supercavitating torpedo. The water in front is boiled to reduce friction so that it can travel faster.
                                            </div>
                                        </div>
                                    </div>              
                                </div>
                
                                <div class="col-lg-7 col-xs-12">
                                    <p class="textwhite-large">Submarine Drag Reduction Study</p>
                                    <p class="textwhite">BAE Systems Summer Internship (2012)</p>
                                    <p>
                                        Before I started university, I spent a summer internship at BAE Systems Maritime completing a project focusing on how to reduce physical drag on the 
                                        new class of nuclear submarines. <br/><br/>
                                        I investigated how changes to the bow, control surfaces, fins, and body of the submarine would affect the hydrodynamics of the submarine. I also 
                                        investigated if it would be possible to inject polymers or micro-bubbles to change the way the water flowed over the submarine's surface. Finally, 
                                        I looked into more radical solutions such as supercavitation &#8212; a technology used in some underwater missiles that involves boiling the water 
                                        in front of the supercavitating body to reduce friction drag.
                                    <br/>
                                    <br/><a class="textlink" href="drag_project.pdf" target="_blank">Read the original report for this project here</a>.</p>
                                </div>
                            </div>

                        </div>
                    </div>


                    <br/>
                    <center><hr class="dividers-small" /></center>
                    <br/>

                    <section id="contacts">
                        <a href="https://x.com/LascellesAlex" class="contacts-icon" target="_blank"><span class="fa fa-twitter"></span></a>
                        <a href="https://github.com/alexlascelles" class="contacts-icon" target="_blank"><span class="fa fa-github"></span></a>
                        <a href="mailto:alexlascelles95@gmail.com" class="contacts-icon"><span class="fa fa-envelope"></span></a>
                        <a href="https://www.linkedin.com/in/alexlascelles/" class="contacts-icon" target="_blank"><span class="fa fa-linkedin"></span></a>
                        <a href="https://www.instagram.com/alexlascelles/" class="contacts-icon" target="_blank"><span class="fa fa-instagram"></span></a>
                    </section>

                    <p id="blue">
                        Copyright Â© Alex Lascelles. Last Updated 2018.
                    </p>

                </div>
            </div>
        </div>
    </body>

</html>