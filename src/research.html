<html>

    <head>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,700,800">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
        
        <title>Alex Lascelles - Research</title>
        <link rel="icon" href="logo_al_white_noborder.png">
        <link rel="stylesheet" href="styles.css">
        <style>
            /* Additional CSS for more centered paragraph text */
            p {
                max-width: 1200px;
                margin: 0 auto; /* Center the paragraph */
                text-align: center; /* Centers the text inside the paragraph */
                padding-bottom: 1em;
            }

            /* Adding padding to the body to account for fixed navigation */
            body {
                padding-top: 6em;
            }
        </style>
    </head>

    <body>
        <div class="container-fluid">
            <div class="fixed-nav">
                <div class="row">
                    <div class="col-xs-12" id="nav-header">
                        <ul>
                            <li class="home-icon"><a href="../index.html"><span class="home-color fa fa-home"></span></a></li>
                            <li><a class="textlink" href="about.html" >About</a></li>
                            <li><a class="blue-title" href="research.html">Research</a></li>
                            <li><a class="textlink" href="projects.html">Personal Projects</a></li>
                            <li><a class="textlink" href="cv.html">CV</a>
                            <li><a class="textlink" href="art.html">Art</a></li>
                            <li><a class="textlink" href="music.html">Music</a></li>
                            <li><a class="textlink" href="contact.html">Contact</a></li>
                        </ul>
                    </div>
                </div>
        
                <hr class="dividers" />
            </div>

            <!--
            <div class="row">
                <div class="col-xl-12 col-xs-12">
                    <p class="blue-title-bigger">Research Interests</p>
            
                    <p>I'm interested in a number of different topics within computational and cognitive neuroscience. Firstly, I'm intrigued by human memory, 
                        especially in the visual and auditory domains. Once we understand exactly how memories are encoded, stored, and recalled, I'm interested 
                        to know if we could then augment these processes. You could imagine a world where we are able to "back up" our memories to the cloud, and 
                        even share our memories with others.
                    </p>
                    <p>
                        It also fascinates me how we process and perceive visual and auditory information concurrently. Often, we use information received in the 
                        auditory domain to help us process a visual cue, and vice versa. Although this sometimes leads to negative biases, I wonder if it can be 
                        utilized in a way that enhances information uptake &#8212; perhaps for use in a learning environment. Additionally, I'm interested in using 
                        imaging techniques such as fMRI and MEG to combine my experience with crossmodal correspondences with memory research. The advent of machine 
                        learning and computer vision also presents many interesting opportunities to combine with neuroscientific data which I'm eager to explore.
                    </p>
                    <p>
                        Although I've switched fields, I've kept my love for Astrophysics! I enjoy reading the latest news, and hope someday I'll have the opportunity 
                        to research supernovae once more.
                    </p>
                </div>
            </div> -->

            <p class="blue-title-bigger">Publications</p>
            <center><hr class="dividers-small" /></center>
            <br>
            <div class="row">
                <div class="col-12">
                    <p style="text-align: start;">
                        Fosco, C., Lahner, B., Pan, B., Andonian, A., Josephs, E., <b>Lascelles, A.</b>, & Oliva, A. (2024). <b>Brain Netflix: Scaling Data to Reconstruct Videos from Brain Signals.</b> <i>European Conference on Computer Vision (pp. 457-474)</i>. Cham: Springer Nature Switzerland. <a class="textlink" href="https://link.springer.com/chapter/10.1007/978-3-031-73347-5_26" target="_blank">Paper</a>
                    </p>
                    <p style="text-align: start;">
                        Lahner, B., Dwivedi, K., Iamshchinina, P., Graumann, M., <b>Lascelles, A.,</b> Roig, G., Gifford, A.T., Pan, B., Jin, S., Murty, N.A.R., Kay, K., Oliva, A.<sup>†</sup>, and Cichy, R.<sup>†</sup> (2024). <b>Modeling short visual events through the BOLD moments video fMRI dataset and metadata.</b> <i>Nature Communications</i>, 15(1), 6241. <a class="textlink" href="LahnerCichy_BOLDMoments_NatureComm_2024.pdf" target="_blank">Paper</a>
                    </p>
                    <p style="text-align: start;">
                        Gifford, A. T., Lahner, B., Saba-Sadiya, S., Vilas, M. G., <b>Lascelles, A.,</b> Oliva, A., Kay, K., Roig, G., and Cichy, R. M. (2023). <b>The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes.</b> <i>arXiv</i>, 2301.03198. <a class="textlink" href="https://doi.org/10.48550/arXiv.2301.03198" target="_blank">arXiv Paper</a> ★ <a class="textlink" href="http://algonauts.csail.mit.edu/" target="_blank">Website</a>
                    </p>
                    <p style="text-align: start;">
                        Monfort, M., Ramakrishnan, K., Andonian, A., McNamara, B., <b>Lascelles, A.,</b> Pan, B., Fan, Q., Gutfreund, D., Feris, R., and Oliva, A. (2021) <b>Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding.</b> <i>IEEE Pattern Analysis and Machine Intelligence (PAMI)</i>, 44(12), 9434-9445. <a class="textlink" href="MonfortOliva_M-MIT_PAMI_2021.pdf" target="_blank">Paper</a> ★ <a class="textlink" href="http://moments.csail.mit.edu/" target="_blank">Website</a>
                    </p>
                    <p style="text-align: start;">
                        Cichy, R.M., Dwivedi, K., Lahner, B., <b>Lascelles, A.,</b> Iamshchinina, P., Graumann, M., Andonian, A., Murty, N.A.R., Kay, K., Roig, G., and Oliva A. (2021). <b>The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of a World in Motion.</b> <i>arXiv</i>, 2104.13714. <a class="textlink" href="https://arxiv.org/pdf/2104.13714v1" target="_blank">arXiv Paper</a> ★ <a class="textlink" href="http://algonauts.csail.mit.edu/2021" target="_blank">Website</a> ★ <a class="textlink" href="https://github.com/Neural-Dynamics-of-Visual-Cognition-FUB/Algonauts2021_devkit" target="_blank">GitHub Code</a>
                    </p>
                    <p style="text-align: start;">
                        Ramakrishnan*, K., Monfort*, M., McNamara, B., <b>Lascelles, A.,</b> Gutfreund, D., Feris, R., and Oliva, A. (2019). <b>Identifying Interpretable Action Units in Deep Networks.</b> <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019) Workshop on Explainable AI.</i> <a class="textlink" href="Ramakrishnan_Identifying_Interpretable_Action_Concepts_in_Deep_Networks_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
                    </p>
                    <p style="text-align: start;">
                        Cichy, R. M., Roig, G., Andonian, A., Dwivedi, K., Lahner, B., <b>Lascelles, A.,</b> Mohsenzadeh, Y., Ramakrishnan, K., and Oliva, A. (2019). <b>The Algonauts Project: A Platform For Communication Between the Sciences of Biological and Artificial Intelligence.</b> <i>arXiv</i>, 1905.05675. <a class="textlink" href="CichyOliva_AlgonautsProject_2019.pdf" target="_blank">Paper</a> ★ <a class="textlink" href="http://algonauts.csail.mit.edu/2019/index.html" target="_blank">Website</a>
                    </p>
                    <p style="text-align: start;">
                        <b>Lascelles, A.</b> and Bhattacharya, J. (2018). <b>Neural Correlates of Crossmodal Correspondence Between Pitch and Visual Motion.</b> <i>MSc Thesis</i>. <a class="textlink" href="msc_thesis_alex_lascelles.pdf" target="_blank">Paper</a> ★ <a class="textlink" href="msc_poster_aal.pdf" target="_blank">Poster for Behavioral Data</a>
                    </p>
                    <p style="text-align: start;">
                        <b>Lascelles, A.,</b> Drake, J., and Garraffo, C. (2017). <b>Realistic MHD Modeling of Wind-Driven Processes in Cataclysmic Variable-Like Binaries.</b> <i>MPhys Thesis</i>. <a class="textlink" href="mphys_thesis_alex_lascelles.pdf" target="_blank">Paper</a> ★ <a class="textlink" target="_blank" href="https://youtu.be/G3KCS8HvReI?t=2110">Video</a>
                    </p>
                    <p style="text-align: start;">
                        <b>Lascelles, A.</b> (2012). <b>Submarine Drag Reduction Study.</b> <i>BAE Systems Maritime Summer Internship</i>. <a class="textlink" href="drag_project.pdf" target="_blank">Paper</a>
                    </p>
                </div>
            </div>

            <p class="blue-title-bigger">Projects</p>
            <center><hr class="dividers-small" /></center>
            <br>

            <div class="row">
                <div class="col-12">

                    <div class="row">
                        <div class="col-xl-6 col-xs-12">
                            <p class="textwhite-large">The Algonauts Project 2023</p> 
                            <p class="textwhite">How the Human Brain Makes Sense of Natural Scenes</p>
                            <p>
                                The 2023 edition of <a class="textlink" href="#Algonauts2019">The Algonauts Challenge</a> focused on explaining responses in the human brain 
                                as participants perceive complex natural visual scenes. Through collaboration with the 
                                <a class="textlink" href="https://naturalscenesdataset.org/">Natural Scenes Dataset (NSD)</a> team, this Challenge ran on the largest suitable 
                                brain dataset available (brain responses from 8 human participants to in total 73,000 different visual scenes) opening new venues for data-hungry 
                                modeling. The challenge was organized in partnership with the 
                                <a class="textlink" href="https://ccneuro.org/" target="_blank">Conference on Cognitive Computational Neuroscience (CCN)</a>.
                                <br><br>
                                At every blink our eyes are flooded by a massive array of photons &#8212; and yet, we perceive the visual world as ordered and meaningful. 
                                The primary target of the 2023 Challenge was predicting human brain responses to complex natural visual scenes. We posed the question: Given a set 
                                of images, how well does your computational model account for the brain activations when a human viewed those images?
                                <br><br>
                                <a class="textlink" href="http://algonauts.csail.mit.edu/index.html" target="_blank">Learn more about Algonauts 2023 here</a>.
                            </p>
                        </div>

                        <div class="col-xl-6 col-xs-12">
                            <iframe src="https://www.youtube.com/embed/KlwSDpxUX6k?vq=hd1080&amp;rel=0" width="560" height="315" title="The Algonauts Project 2023: Challenge &amp; development kit tutorial walk-through" frameborder="0" allowfullscreen="" data-dashlane-frameid="350"></iframe>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-6 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="GoalGrounding.gif" alt="goalgrounding" height="400">
                                <div class="middle">
                                    <div class="thetext">
                                        Example of the online tool we built.
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="col-xl-6 col-xs-12">
                            <p class="textwhite-large">Goal Grounding Project</p>
                            <p class="textwhite">Group collaboration between MIT and Facebook Reality Labs studying the goals of videos</p>
                            <p>
                                The aim of this project was to collect annotations on egocentric (first-person) videos that have a specific goal which is being carried out. 
                                This results in visual ground truth data for when a certain goal is taking place within a video. This would be useful for a number of different 
                                applications, for example, an AI assistant predicting and helping with the next action/goal a human will encounter.
                                <br><br>
                                To do this, our team at MIT collaborated with a team at Meta to build an interface to collect data from participants online, selected suitable data sets 
                                to sample from, collected the data using the Prolific platform, and analyzed the data. The interface was a useful online tool that allowed a user to quickly 
                                select portions of video while watching, and was flexible enough to be applied to other research aims.
                                <br><br>
                                <a class="textlink" href="contact.html" target="_blank">If you'd like to know more about this project please get in touch</a>.
                            </p>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large">The Algonauts Project 2021</p> 
                            <p class="textwhite">How the Human Brain Makes Sense of a World in Motion</p>
                            <p>
                                The 2nd installment of <a class="textlink" href="#Algonauts2019">The Algonauts Challenge</a> was centered around video perception and event understanding, 
                                and was organized in partnership with the <a class="textlink" href="https://ccneuro.org/" target="_blank">Conference on Cognitive Computational Neuroscience (CCN)</a>. 
                                The challenge focused on explaining responses in the human brain as subjects watched short video clips of everyday events.
                                <br><br>
                                For this release, in addition to building the website, I was also involved in the collection of the data set &#8212; creating the video data set (1102 3-second videos), 
                                learning fMRI techniques, scanning 10+ participants, and making all the data available to the public.
                                <br><br>
                                <a class="textlink" href="http://algonauts.csail.mit.edu/2021/index.html" target="_blank">Learn more about Algonauts 2021 here</a>.
                            </p>
                        </div>

                        <div class="col-xl-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="algonauts2021.png" alt="algonauts2021" height="275">
                            </div>
                            <div class="picframe">
                                <img class="pic" src="ap2021_data_figure_training_v2_compressed.gif" alt="algonauts2021_gif" height="200">
                                <div class="middle">
                                    <div class="thetext">
                                        The data set consisted of a set of 1000 3-second videos of everyday events, for which we also provided the fMRI brain 
                                        data associated with humans viewing these videos.
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-6 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="PaperSummary.png" alt="paper_summary" height="400">
                                <div class="middle">
                                    <div class="thetext">
                                        Example of one of the academic paper summaries.
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="col-xl-6 col-xs-12">
                            <p class="textwhite-large">Computational Neuroscience Paper Summaries</p> 
                            <p>
                                I created one-slide summaries of 100+ computational visual neuroscience academic papers, and other resources. It was made so that you could quickly understand the 
                                gist of the paper/resources, and easily come back to it later to remember what it was about. They included a short section on the background, info 
                                on what the researchers did, what they found, and any other important information.
                            </p>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row" id="Algonauts2019">
                        <div class="col-xl-5 col-xs-12">
                            <p class="textwhite-large">The Algonauts Project 2019</p> 
                            <p class="textwhite">Explaining the Human Visual Brain: Workshop and Challenge</p>
                            <p>
                                While at MIT, I helped to build The Algonauts Project &#8212; 
                                an initiative bringing biological and artificial intelligence researchers together on a 
                                common platform to exchange ideas and advance both fields. Inspired by the astronauts' exploration of space, "algonauts" explore human and artificial intelligence 
                                with state-of-the-art algorithmic tools. <a class="textlink" href="http://algonauts.csail.mit.edu/2019/index.html" target="_blank">Our first challenge and workshop, 
                                "Explaining the Human Visual Brain"</a>, focused on building computer vision models that simulate how the brain sees and recognizes <i>objects</i>, a topic that 
                                has long fascinated neuroscientists and computer scientists.
                                <br><br>
                                Across two competition tracks, fMRI and MEG, we gave participants a set of images consisting of everyday objects and the corresponding brain activity recorded 
                                while human subjects viewed those images. Participants competed to devise computational models that best predicted the brain activity of a brand new set of images. 
                                The winners were announced at a workshop that we organized at MIT, featuring guest speakers who are some of the leading experts in the field of computational 
                                neuroscience of vision, from institutions such as Harvard, UC Berkeley, Columbia, FU Berlin, and DeepMind.
                                <br><br>
                                <a class="textlink" href="http://algonauts.csail.mit.edu/archive.html" target="_blank">Learn more about The Algonauts Project and the 2019 edition here, 
                                on the webpages that I built to explain and distribute the challenges and workshops</a>.
                            </p>
                        </div>
                        
                        <div class="col-xl-7 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="algonauts2019.png" alt="algonauts2019" height="275">
                            </div>
                            <div class="picframe">
                                <img class="pic" src="challenge_summary_v3.png" alt="algonauts2019_summary" height="375">
                                <div class="middle">
                                    <div class="thetext">
                                        Summary figure explaining the 2019 challenge. 
                                        <br><br>
                                        The data set consisted of a set of 92 and 118 images, for which we also provided the fMRI and MEG brain 
                                        data associated with humans viewing these images.
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>


                    <div class="row">
                        <div class="col-xl-6 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="msc.png" alt="msc" height="375"/>
                                <div class="middle">
                                    <div class="thetext">
                                        Topoplots showing brain activity during trials where the spoken English words "up" and "down" did bias participants' perception of 
                                        motion (congruent) and where they did not (incongruent).
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="col-xl-6 col-xs-12">
                            <p class="textwhite-large">MSc Thesis (2018):</p>
                            <p class="textwhite">Neural Correlates of Crossmodal Correspondence Between Pitch and Visual Motion</p>
                            <p>Supervisor: Prof Joydeep Bhattacharya FRSA</p>
                            <p>
                                Our senses are not independent entities &#8212; instead, they work together to build our realities. When one sense influences another, we call this 
                                phenomenon <i>crossmodal correspondence</i>. I studied a particular crossmodal correspondence in which ascending/descending pitches as well as the 
                                spoken words "up"/"down" (auditory domain) were able to bias our perception of visual motion (visual domain).
                                <br><br>    
                                I learned MATLAB so I could design and conduct an EEG experiment in which the participant judged the direction of motion of an ambiguous (same 
                                upwards and downwards components) moving grating whilst listening to the auditory stimuli. Using behavioural and ERP analysis, I showed that 
                                auditory stimuli were enough to bias peoples' perception of visual motion &#8212; an ascending tone caused more people to perceive upwards motion, 
                                when in fact there was no overall movement in either direction. This was the first time that this effect was studied using brain imaging techniques.
                                <br><br>
                                <a class="textlink" href="msc_thesis_alex_lascelles.pdf" target="_blank">Read my MSc thesis here</a> or 
                                <a class="textlink" href="msc_poster_aal.pdf" target="_blank">view my poster for the behavioural data here</a>.
                            </p>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-6 col-xs-12">
                            <p class="textwhite-large">Speed Reader</p> 
                            <p class="textwhite">Anvil Hack 2018</p>
                            <p>
                                While at Goldsmiths, I participated in a number of different hackathons including Anvil Hack. During this event, I worked in a team of 3 to make a platform 
                                that allows you to convert text into a form that was much more quickly readable.  
                                <a class="textlink" href="https://youtu.be/QlxADlERFkA" target="_blank">We were inspired by this advert by Honda</a>. Perhaps in the future, devices that 
                                use this technology might be commonplace.
                            <!-- You can try it out by visiting our <a class="textlink" href="https://github.com/lnfiniteMonkeys/Anvil_IV" target="_blank">GitHub repository</a>. -->
                            </p>
                        </div>
                        
                        <div class="col-xl-6 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="speedreader.gif" alt="speed_reader" height="400">
                                <div class="middle">
                                    <div class="thetext">
                                        An example of this technology in practice. Your reading speed is greatly increased when your eyes don't have to make multiple saccades from word to word, 
                                        but can instead remain focused in one place.
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-5 col-xs-12">                            
                            <div class="picframe">
                                <img class="pic" src="3d_cinematic.png" alt="MHD" height="400">
                                <div class="middle">
                                    <div class="thetext">
                                        A close up of one of my simulations. <br/><br/>The magnetic field lines are represented in grey and wind speed by the 
                                        red/black color map. The blue color map shows mass density (effectively tracing accretion).
                                    </div>
                                </div>
                            </div>

                            <div class="video-container">
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/G3KCS8HvReI?si=c1AQrvF3ziyUuotL&amp;start=2110" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                            </div>
                        </div>

                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large" style="margin-top:1em">MPhys Thesis (2017):</p>
                            <p class="textwhite">Realistic MHD Modeling of Wind-Driven Processes in Cataclysmic Variable-Like Binaries</p>
                            <p>Supervisors: Dr Cecilia Garraffo and Dr Jeremy Drake</p>
                            <p>
                                When main sequence stars such as our Sun orbit a star known as a white dwarf, they can form something called a cataclysmic variable-like binary. The 
                                charged ions contained within the magnetic fields of these stars creates a complex magnetic <i>wind</i> structure as the two bodies orbit each other. 
                                When this wind structure decouples from the system it carries away mass and angular momentum, which impacts how the stars spin and how their orbits evolve. 
                                <br><br>
                                Using 3-D magnetohydrodynamic simulations, I explored the effects of orbital separation and magnetic field configuration on these mass and angular momentum 
                                loss rates for binary systems. The current models didn't include the magnetic fields of the stars. It was hypothesized that including the magnetic field of 
                                the white dwarf in the binary system could alter these rates. I found that when including these magnetic fields, the mass and angular momentum loss rates 
                                dropped by factors of 4 and 6 respectively, suggesting that the current models for these systems should be amended to include the magnetic field.
                                <br><br>
                                <a class="textlink" href="mphys_thesis_alex_lascelles.pdf" target="_blank">Read my MPhys thesis here</a> 
                                or <a class="textlink" href="https://youtu.be/G3KCS8HvReI?t=2110" target="_blank">watch a 20-min video of my thesis defense here</a>, given at the Harvard-Smithsonian 
                                Center for Astrophysics High Energy Seminar.
                            </p>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large">Hack @ Brown 2017</p> 
                            <p class="textwhite">Saguaro: An open source, extensible home automation platform</p>
                            <p>
                                I attended a hackathon at Brown University where I worked within a team of 5 to design and build an IOT platform called Saguaro. Saguaro allows 
                                hardware devices to be controlled by a simple web interface. 
                                <br><br>
                                Activating hardware in your home &#8212; lights, windows, anything you can run a wire to &#8212; is as simple as pushing a button. Additionally, 
                                Saguaro learns your schedule, and over time is able to anticipate your actions and automate your home for you.
                            </p>
                            <p>
                                <a class="textlink" href="https://devpost.com/software/saguaro" target="_blank">Learn more about this project here</a>.</p>
                        </div>

                        <div class="col-xl-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="saguaro.jpg" alt="saguaro" height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        A prototype of our IoT home! Built using an Arduino.
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>
                
                    <div class="row">
                        <div class="col-xl-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="orbits.png" alt="orbits"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        Some of the different orbits that I discovered, shown in stationary (left) and rotating (right) frames of reference.
                                    </div>
                                </div>
                            </div>
                            
                            <div class="picframe">
                                <img class="pic" src="binary.jpg" alt="binary"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        Artist's rendition of an Earth-like planet orbiting a binary star system.
                                    </div>
                                </div>
                            </div>
                            <br/>
                            <p class="text-small">Bottom image credit: Lynette Cook / <a class="textlink" href="https://extrasolar.spaceart.org/" target="_blank">extrasolar.spaceart.org</a></p>
                        </div>

                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large">What Do Planetary Orbits around Binary Star Systems Look Like?</p> 
                            <p class="textwhite">Computing Project #1 (2016)</p>
                            <p>
                                Incredibly, almost half of the star systems that we see in the sky contain multiple stars! This project aimed to simulate planetary orbits around 
                                a binary (two) star system. This is important to study because such simulations could be used to detect habitable planets outside our solar system (exoplanets). 
                                For life to exist, the planet on which it occurs must keep a stable orbit over long time-scales. This is similar to the novel "The Three-Body Problem" 
                                by Liu Cixen, now made into a <a class="textlink" href="https://www.youtube.com/watch?v=mogSbMD6EcY&ab_channel=Netflix" target="_blank">Netflix series</a>. <br><br>
                                To find stable orbits, I used a Runge-Kutta approach to solve the differential equations involved in such a 3-body system and tested various initial 
                                velocities and separations. I found many p- and s-type orbits (the two species of stable planetary orbit), as well as some chaotic orbits, 
                                and discovered whether they possessed a habitable zone where the planet could sustain life.
                            </p>
                            <p>
                                <a class="textlink" href="binary_project.pdf" target="_blank">Read my report on this project here</a>.</p>
                            <br/>
                            <br/>                        
                        </div>
                    </div>
                    
                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">
                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large">The Structure of White Dwarf Stars</p> 
                            <p class="textwhite">Computing Project #2 (2016)</p>
                            <p>
                                White dwarf (WD) stars are extremely dense objects &#8212; the only thing preventing them from collapsing into a black hole is a force called 
                                <a class="textlink" href="https://en.wikipedia.org/wiki/Electron_degeneracy_pressure" target="_blank">electron degeneracy pressure</a>, which 
                                has to do with the fact that electrons cannot be pushed close enough to occupy the same energy state. WDs are very important within astronomy 
                                &#8212; they appear in many areas of study, including galaxy formation, stellar evolution, and supernovae. Since over 95% of the stars we 
                                observe will end their lives as WDs, knowing how they function and how their parameters behave is crucial. <br><br>
                                In this project, I created a model for determining the mass-radius variation in WDs, and ultimately found the critical mass of a WD 
                                (<a class="textlink" href="https://en.wikipedia.org/wiki/Chandrasekhar_limit" target="_blank">Chandrasekhar limit</a>), beyond which the 
                                electron degeneracy pressure can no longer support the star and it must collapse into a neutron star or a black hole.
                            </p>
                            <p ><a class="textlink" href="wd_project.pdf" target="_blank">Read more about this project here</a>.</p>                      
                        </div>

                        <div class="col-xl-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="white_dwarf2.jpg" alt="wd"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        Artist's rendition of a white dwarf star.
                                    </div>
                                </div>
                            </div>
                            
                            <div class="picframe">
                                <img class="pic" src="wd.png" alt="myWD"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        Radius vs Mass: Plot shows how parameters of real WDs (black and red) compare to my model of Carbon and Iron WDs.
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>
                
                    <div class="row">
                        <div class="col-xl-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="eclipsing_binary_rendition.webp" alt="binary"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        Artist's rendition of an eclipsing binary star system. It consists of a white dwarf star (left) orbiting 
                                        a main sequence star (right) while accreting matter from it via a process known as Roche lobe overflow.
                                    </div>
                                </div>
                            </div>

                            <div class="picframe">
                                <img class="pic" src="eclipsing_binary.png" alt="orbits"  height="300">
                                <div class="middle">
                                    <div class="thetext">
                                        A reduced I-band image of the eclipsing binary star system "IR Com" (circled) with FOV 10x10 arcmin.
                                    </div>
                                </div>
                            </div>

                            <p class="text-small">Top image credit: Kevin Burdge / <a class="textlink" href="https://communities.springernature.com/posts/discovering-the-shortest-orbital-period-eclipsing-binary-known" target="_blank">Article</a></p>
                        </div>

                        <div class="col-xl-7 col-xs-12">
                            <p class="textwhite-large">How Do You Find the Period of an Eclipsing Binary Star System?</p> 
                            <p class="textwhite">Astrophysics Project at the Teide Observatory, Tenerife (2015)</p>
                            <p>
                                As part of my masters degree, we spent a week in Tenerife collecting data with professional telescopes to conduct an experiment 
                                on an area of astrophysics of our choosing. I chose to research eclipsing binaries &mdash; these are a 2-star system consisting of a white 
                                dwarf orbiting a main sequence star (similar to our Sun). The white dwarf is much more massive and therefore accretes (steals) matter 
                                from the other star. 
                                <br><br>
                                When we view them using telescopes it appears to be just a single point of light because we are so far away. 
                                But since the two stars are orbiting each other, if viewed from the right angle (plane of rotation), one star will periodically pass in front of the other, resulting in a dimming of the total 
                                brightness. This dimming of light can be measured, and from that we can work out the period (time for the stars to completely orbit 
                                one another). In this project, I measured the dimming of the binary over time to work out the period (~2 hrs). If you are interested 
                                in learning more about eclipsing binaries, <a href="https://www.youtube.com/watch?v=gmzmNDzUHEk&ab_channel=UNLAstronomy" target="_blank">this is a great resource</a>!
                                <br><br>
                                <a href="eclipsing_binary_project_teide.pdf">Read my report on this project here</a>.
                            </p>                      
                        </div>
                    </div>
                    
                    <br/>
                    <center><hr class="dividers-smallish" /></center>
                    <br/>

                    <div class="row">        
                        <div class="col-lg-7 col-xs-12">
                            <p class="textwhite-large">How Can Submarines Reduce Drag?</p>
                            <p class="textwhite">BAE Systems Summer Internship (2012)</p>
                            <p>
                                Before I started university, I spent a summer internship at BAE Systems Maritime completing a project focusing on how to reduce physical drag on the 
                                new class of nuclear submarines. <br/><br/>
                                I investigated how changes to the bow, control surfaces, fins, and body of the submarine would affect the hydrodynamics of the submarine. I also 
                                investigated if it would be possible to inject polymers or micro-bubbles to change the way the water flowed over the submarine's surface. Finally, 
                                I looked into more radical solutions such as supercavitation &#8212; a technology used in some underwater missiles that involves boiling the water 
                                in front of the supercavitating body to reduce friction drag.
                            <br/>
                            <br/><a class="textlink" href="drag_project.pdf" target="_blank">Read the report here</a>.</p>
                        </div>

                        <div class="col-lg-5 col-xs-12">
                            <div class="picframe">
                                <img class="pic" src="submarine2.png" alt="submarine2" height="300"> 
                                <div class="middle">
                                    <div class="thetext">
                                        An exmaple of a supercavitating torpedo. The water in front is boiled to reduce friction so that it can travel faster.
                                    </div>
                                </div>
                            </div>              
                        </div>
                    </div>

                </div>
            </div>


            <br/>
            <center><hr class="dividers-small" /></center>
            <br/>

            <section id="contacts">
                <a href="https://x.com/LascellesAlex" class="contacts-icon" target="_blank"><span class="fa fa-twitter"></span></a>
                <a href="https://github.com/alexlascelles" class="contacts-icon" target="_blank"><span class="fa fa-github"></span></a>
                <a href="mailto:alexlascelles95@gmail.com" class="contacts-icon"><span class="fa fa-envelope"></span></a>
                <a href="https://www.linkedin.com/in/alexlascelles/" class="contacts-icon" target="_blank"><span class="fa fa-linkedin"></span></a>
                <a href="https://www.instagram.com/alexlascelles/" class="contacts-icon" target="_blank"><span class="fa fa-instagram"></span></a>
            </section>

            <p id="blue">
                Copyright © Alex Lascelles. Last Updated Jul 2025.
            </p>

        </div>
    </body>
</html>
